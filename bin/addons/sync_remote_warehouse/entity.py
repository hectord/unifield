import uuid
import base64
from zipfile import ZipFile
from cStringIO import StringIO
import csv
import sys
import copy
import os
from datetime import datetime
import re

from osv import osv, fields, orm
from tools.translate import _
from sync_client import sync_process
import release
import pooler
from sync_common import get_md5


class Entity(osv.osv):

    _DEBUG = True

    _inherit = 'sync.client.entity'

    _columns = {
        # used to ignore all data older than this date when syncing as it is already in the db
        'clone_date': fields.datetime('Backup Date And Time', help='The date that the Central Platform database was backed up to provide the seed data for the Remote Warehouse'),
        
        # the step of the synchronisation process - first_sync, pull_performed, push_performed
        'usb_sync_step': fields.selection((('first_sync','First Synchronisation'), ('pull_performed','Pull Performed'), ('push_performed', 'Push Performed')), 'USB Synchronisation Step'), 
        
        # used to make sure user does not try to import old data
        'usb_last_push_date': fields.datetime('Last Push Date', help='The date and time of the last Push'),
        'usb_last_push_file': fields.binary('Last Push File', help='The zip file that was generated by the last Push'),

        # last tarball of patches used by the last push of updates
        'usb_last_tarball_patches': fields.binary('Last Push File', help='The tarball that was generated by the last Push'),

        # US-16: This field keeps the sequence of the latest pull file         
        'rw_pull_sequence': fields.integer(string='Sequence of the pull zip file'),
    }
    
    _defaults = {
        'usb_sync_step': 'first_sync',
        'usb_instance_type': '',
        'rw_pull_sequence': 0,
    }
    
    def __init__(self, pool, cr):
        super(Entity, self).__init__(pool, cr)
        self.usb_pull_update_received_model_name = 'sync_remote_warehouse.update_received'
        self.usb_pull_message_received_model_name = 'sync_remote_warehouse.message_received'
        
        self.usb_pull_update_rule_file = 'update_rules.txt'
        self.usb_pull_message_rule_file = 'message_rules.txt'
    
        self.usb_pull_files = [
            '%s.csv' % self.usb_pull_update_received_model_name,
            '%s.csv' % self.usb_pull_message_received_model_name,
        ]
        self.usb_pull_files_rw = [
            self.usb_pull_update_rule_file,
            self.usb_pull_message_rule_file,
        ]

    def _check_md5(self, md5_data, data, filename):
        if md5_data[filename] != get_md5(data):
            raise osv.except_osv(_("Error!"), _('Zip file is corrupted, file %s: checksum does not match ') % (filename, ))
    def _usb_change_sync_step(self, cr, uid, step):
        if self._DEBUG:
            return True

        entity = self.get_entity(cr, uid)
        return self.write(cr, uid, entity.id, {'usb_sync_step': step})
        
    @sync_process(step='data_push', need_connection=False, defaults_logger={'usb':True})
    def usb_push(self, cr, uid, push_file_name, context):
        """
        Create updates, create message, package into zip, attach to entity and increment entity usb sync step
        """
        context = context or {}
        context.update({'offline_synchronization' : True})
        logger = context.get('logger')
        
        session = str(uuid.uuid1())
        entity = self.get_entity(cr, uid, context=context)
        self.write(cr, uid, [entity.id], {'session_id' : session}) 
        
        # get update and message data
        self.usb_push_create_update(cr, uid, session, context=context)
        logger.switch('msg_push', 'in-progress')
        self.usb_push_create_message(cr, uid, context=context)
        
        # compress into zip
        updates_count, deletions_count, messages_count, update_ids, message_ids = self.usb_push_create_zip(cr, uid, push_file_name, context=context)
        
        # cleanup
        self.usb_push_validate(cr, uid, update_ids, context=context)
        self.write(cr, uid, entity.id, {'session_id' : ''}, context=context)
        
        # advance step if there was something to push
        self._usb_change_sync_step(cr, uid, 'push_performed')
        
        # return 
        return (updates_count, deletions_count, messages_count)
    
    def usb_push_create_zip(self, cr, uid, push_file_name, context=None):
        """
        Create packages out of all total_updates marked as "to send", format as CSV, zip and attach to entity record 
        """
        
        update_csv_contents = []
        message_csv_contents = []
        context = context or {}
        total_updates = total_deletions = total_messages = 0 
        update_ids = []
        message_ids = []
        entity = self.get_entity(cr, uid)
        
        logger = context.get('logger', None)
        logger_index = logger.append()

        # Get the sequence of the push file before creating the zip file, if first time, create it
        def get_pull_sequence():
            seq_pool = self.pool.get('ir.sequence')
            seq = seq_pool.get(cr, uid, 'rw.push.seq')
            # value must be int for simplifying at the partner instance   
            return int(seq)

        def generate_header(push_file_name):
            # US-16: Now get the sequence of the push from this instance
            seq = get_pull_sequence()
            
            # Add the pull sequence into the header of the zip file
            header = {'release':release.version[:-16] or release.version, 'file_name': push_file_name, 'rw_pull_sequence': seq}
            revisions = self.pool.get('sync_client.version')
            if revisions:
                entity = self.get_entity(cr, uid, context=context)
                rev_ids = revisions.search_installed_since(cr, uid,
                    entity.usb_last_push_date, context=context)
                header['patches'] = map(
                    lambda p: (p.name, p.sum, p.date, p.importance, p.comment),
                    revisions.browse(cr, uid, rev_ids, context=context))
            return unicode(header)

        def update_create_package():
            return self.pool.get('sync_remote_warehouse.update_to_send').create_package(cr, uid)

        def update_add_and_mark_as_sent(ids, packet):
            """
            add the package contents to the update_csv_contents dictionary and mark the package as sent
            @return: (number of total_updates, number of delete_sdref rules)
            """
            rule_id = self.pool.get('sync.client.rule').search(cr, uid, [('server_id','=',packet['rule_id'])])
            if not rule_id:
                raise osv.except_osv('Cannot find rule', 'Cannot find rule with server_id %s' % packet['rule_id'])
            rule = self.pool.get('sync.client.rule').browse(cr, uid, rule_id[0])
            
            # create header row if needed
            columns = ['source', 'model', 'version', 'sequence_number', 'rule_sequence', 'fields', 'values', 'sdref', 'is_deleted']
            if not update_csv_contents:
                update_csv_contents.append(columns)
            
            # insert update data
            for update in packet['load']:
                update_csv_contents.append([
                    entity.name,
                    packet['model'], # model
                    update['version'],
                    rule.sequence_number,
                    rule.server_id,
                    packet['fields'],
                    update['values'],
                    update['sdref'],
                    False,
                ])
                
            # insert delete data
            for delete_sdref in packet['unload']:
                update_csv_contents.append([
                    entity.name,
                    packet['model'],
                    '',
                    '',
                    '',
                    '',
                    '',
                    delete_sdref,
                    True,
                ])
            
            # mark updates_to_send as sent and return number of records processed
            self.pool.get('sync_remote_warehouse.update_to_send').write(cr, uid, ids, {'sent' : True}, context=context)
            return (len(packet['load']), len(packet['unload']))
        
        def message_add_and_mark_as_sent(packet):
            columns = ['source','remote_call','identifier','arguments']
            
            if not message_csv_contents:
                message_csv_contents.append(columns)
            
            # insert message data
            for message in packet:
                message_csv_contents.append([
                    message['source'],
                    message['remote_call'],
                    message['id'],
                    message['arguments'],
                ])
                
            # mark updates_to_send as sent and return number of records processed
            messages_pool.packet_sent(cr, uid, packet, context=context) 
            return len(packet)
        
        _update_rules_serialization_mapping = {
            'id' : 'server_id',
            'name' : 'name',
            'owner_field' : 'owner_field',
            'model' : 'model',
            'domain' : 'domain',
            'sequence_number' : 'sequence_number',
            'included_fields' : 'included_fields',
            'can_delete' : 'can_delete',
            'direction_usb' : 'direction_usb',
            'type': 'type',
        }
    
        def _serialize_update_rule(update_rule_pool, cr, uid, ids, context=None):
            rules_data = []
            for rule in update_rule_pool.browse(cr, uid, ids, context=context):
                rules_data.append(dict(
                    (data, rule[column]) for column, data
                        in _update_rules_serialization_mapping.items()
                ))
            return rules_data
        
        _message_rules_serialization_mapping = {
            'server_id': 'server_id',      
            'name': 'name' ,
            'model': 'model',
            'domain': 'domain',
            'filter_method': 'filter_method',
            'sequence_number': 'sequence_number',
            'remote_call': 'remote_call',
            'arguments': 'arguments',
            'destination_name': 'destination_name',
            'active': 'active',
            'type' : 'type'
        }
        
        def _serialize_message_rule(self, cr, uid, ids, context=None):
            rules_data = []
            for rule in self.browse(cr, uid, ids, context=context):
                rules_data.append(dict(
                    (data, rule[column]) for column, data
                        in _message_rules_serialization_mapping.items()
                ))
            return rules_data

        updates_todo = len(self.pool.get('sync_remote_warehouse.update_to_send').search(cr, uid, [('sent','=',False)], context=context))
        messages_todo = len(self.pool.get('sync_remote_warehouse.message_to_send').search(cr, uid, [('sent','=',False)], context=context))
        
        if not updates_todo:
            logger.switch('data_push', 'ok')
            
        if not messages_todo:
            logger.switch('msg_push', 'ok')
        
        ################################################### 
        ################# create updates ##################
        ###################################################

        logger.replace(logger_index, _('Update(s) to package: %d') % updates_todo)
        
        if updates_todo:        
            package = update_create_package()
            
            while package:
                # add the package to the update_csv_contents dictionary and mark it has 'sent'
                updates, deletions = update_add_and_mark_as_sent(*package)
                total_updates += updates
                total_deletions += deletions
                update_ids += package[0]
                package = update_create_package()
                
            # finished all packages so update logger
            if total_updates or total_deletions:
                logger.replace(logger_index, _("Update(s) packaged: %d update(s) and %d deletion(s) = %d in total") % (total_updates, total_deletions, (total_updates + total_deletions)))
        
        ################################################### 
        ################# create messages #################
        ###################################################
        
        logger_index = logger.append(_('Message(s) to package: %d' % messages_todo))

        if messages_todo:
            # prepare packets
            messages_pool = self.pool.get('sync_remote_warehouse.message_to_send')
            package = messages_pool.get_message_packet(cr, uid, context=context)
            total_messages = len(package)
            message_ids += package[0]
            
            # add packet to csv data list
            message_add_and_mark_as_sent(package)
            logger.replace(logger_index, _("Message(s) packaged: %d") % (total_messages))
        
        ################################################### 
        ################# update rules ####################
        ###################################################
        
        # create update rules file to send to remote warehouse if instance is central platform
        if entity.usb_instance_type == 'central_platform':
             
            update_rule_pool = self.pool.get('sync.client.rule')
            update_rule_ids = update_rule_pool.search(cr, uid, [('type','=','USB'),'|',('direction_usb','=','rw_to_cp'),('direction_usb','=','bidirectional')])
            update_rules_serialized = _serialize_update_rule(update_rule_pool, cr, uid, update_rule_ids, context=context)
            
            update_rules_string_io = StringIO()
            update_rules_string_io.write(str(update_rules_serialized))
        
        ################################################### 
        ################# message rules ###################
        ###################################################
        
        # create message update rules file to send to remote warehouse if instance is central platform
        if entity.usb_instance_type == 'central_platform': 
            message_rule_pool = self.pool.get('sync.client.message_rule')
            message_rule_ids = message_rule_pool.search(cr, uid, [('type','=','USB')])
            message_rules_serialized = _serialize_message_rule(message_rule_pool, cr, uid, message_rule_ids, context=context)
            
            message_rules_string_io = StringIO()
            message_rules_string_io.write(str(message_rules_serialized))
        
        ################################################### 
        ################# create zip ######################
        ###################################################
        
        try:
            # create update csv file
            update_csv_string_io = StringIO()
            update_csv_writer = csv.writer(update_csv_string_io, delimiter=',', quotechar='"', quoting=csv.QUOTE_ALL)
            for data_row in update_csv_contents:
                update_csv_writer.writerow(data_row)
                
            # create message csv file
            message_csv_string_io = StringIO()
            message_csv_writer = csv.writer(message_csv_string_io, delimiter=',', quotechar='"', quoting=csv.QUOTE_ALL)
            for data_row in message_csv_contents:
                message_csv_writer.writerow(data_row)
                    
            # compress update and message csv file into zip
            zip_file_string_io = StringIO()
            zip_base64_output = StringIO()
            zip_file = ZipFile(zip_file_string_io, 'w')
            
            header_data = generate_header(push_file_name)
            md5_data = "header: %s\n" % get_md5(header_data)
            zip_file.writestr('header', header_data)

            update_csv_string_io_data = update_csv_string_io.getvalue()
            md5_data += "sync_remote_warehouse.update_received.csv: %s\n" % get_md5(update_csv_string_io_data)
            zip_file.writestr('sync_remote_warehouse.update_received.csv', update_csv_string_io_data)

            message_csv_string_io_data = message_csv_string_io.getvalue()
            md5_data += "sync_remote_warehouse.message_received.csv: %s\n" % get_md5(message_csv_string_io_data)
            zip_file.writestr('sync_remote_warehouse.message_received.csv', message_csv_string_io_data)
            
            # compress update and message rules into zip file
            if entity.usb_instance_type == 'central_platform':
                update_rules_string_io_data = update_rules_string_io.getvalue()
                md5_data += "update_rules.txt: %s\n" % get_md5(update_rules_string_io_data)
                zip_file.writestr('update_rules.txt', update_rules_string_io_data)

                message_rules_string_io_data = message_rules_string_io.getvalue()
                md5_data += "message_rules.txt: %s\n" % get_md5(message_rules_string_io_data)
                zip_file.writestr('message_rules.txt', message_rules_string_io_data)

            zip_file.writestr('md5', md5_data)
            zip_file.close()

            # add to entity object
            zip_file_contents = zip_file_string_io.getvalue()
            zip_base64 = base64.encodestring(zip_file_contents)
    
            # clean up
            zip_base64_output.close()
            update_csv_string_io.close()
            message_csv_string_io.close()
            if entity.usb_instance_type == 'central_platform':
                update_rules_string_io.close()
                message_rules_string_io.close()
            zip_file_string_io.close()
    
            # attach file to entity and delete 
            self.write(cr, uid, entity.id, {'usb_last_push_file': zip_base64, 'usb_last_push_date': datetime.now()})

            logger.switch('data_push', 'ok')
            logger.switch('msg_push', 'ok')            
            logger.switch('status', 'ok')
        except Exception, e:
            logger.append(_('Error while creating zip file: %s') % str(e))
            logger.switch('data_push', 'failed')
            logger.switch('msg_push', 'failed')
            logger.switch('status', 'failed')
            raise
        
        return (total_updates, total_deletions, total_messages, update_ids, message_ids)
    
    def usb_push_create_update(self, cr, uid, session, context=None):
        """
        Create update_to_send for a USB synchronization and return a browse of all to-send updates
        """
        
        # init
        context = context or {}
        context.update({
            'update_to_send_model': 'sync_remote_warehouse.update_to_send', 
            'last_sync_date_field': 'usb_sync_date'
        })
        
        logger = context.get('logger', None)
        logger_index = logger.append()
        
        entity = self.get_entity(cr, uid, context)
        update_pool = self.pool.get('sync_remote_warehouse.update_to_send')
        updates_count = 0
        
        # search for rules         
        rule_search_domain = [('type','=','USB')]
        if entity.usb_instance_type == 'central_platform':
            rule_search_domain += ['|',('direction_usb','=','cp_to_rw'),('direction_usb','=','bidirectional')]
        else:
            rule_search_domain += ['|',('direction_usb','=','rw_to_cp'),('direction_usb','=','bidirectional')]
             
        rule_ids = self.pool.get('sync.client.rule').search(cr, uid, rule_search_domain, context=context)
        
        # create updates for rules found
        if rule_ids:
            for rule_id in rule_ids:
                updates_count += sum(update_pool.create_update(cr, uid, rule_id, session, context=context))
                logger.replace(logger_index, _('Update(s) created: %d') % updates_count)
                    
            if updates_count:
                cr.commit()
        else:
            logger.replace(logger_index, _('No applicable update rules found'))
            
        return len(update_pool.search(cr, uid, [('sent','=',False)]))
    
    def usb_push_create_message(self, cr, uid, context=None):
        context = context or {}
        message_pool = self.pool.get('sync_remote_warehouse.message_to_send')
        rule_pool = self.pool.get("sync.client.message_rule")
        logger = context.get('logger')
        logger_index = logger.append()
        entity = self.get_entity(cr, uid, context)

        messages_count = 0
        message_direction = entity.usb_instance_type == 'central_platform' and \
            ['|', ('direction_usb', '=', 'cp_to_rw'), ('direction_usb', '=', 'bidirectional')] or \
            ['|', ('direction_usb', '=', 'rw_to_cp'), ('direction_usb', '=', 'bidirectional')]
        rule_ids = rule_pool.search(cr, uid, [('type','=','USB')] + message_direction, order='sequence_number',  context=context)
        if rule_ids:
            for rule in rule_pool.browse(cr, uid, rule_ids, context=context):
                order = "id asc"
                if 'usb_create_partial_int_moves' in rule.remote_call or 'usb_create_partial_in' in rule.remote_call:
                    # US-327: Don't create these 2 types of messages if the current instance is CP
                    if entity.usb_instance_type == 'central_platform':
                        continue
                    else:
                        # For this INT sync, create messages ordered by the date_done to make sure that the first INTs will be synced first
                        order = 'date_done asc'
                messages_count += message_pool.create_from_rule(cr, uid, rule, order, context=context)
                logger.replace(logger_index, _('Message(s) created: %s') % messages_count)
        
            if messages_count:
                cr.commit()
        else:
            logger.replace(logger_index, _('No applicable message rules found'))
        
        # return number of messages to send
        return len(message_pool.search(cr, uid, [('sent','=',False)], context=context))

    # UF-2483: Run the initial push messages to create all RW messages, put them into the sent box, to avoid sending them on the first RW sync 
    def usb_push_create_message_initial(self, cr, uid):
        context = {}
        message_pool = self.pool.get('sync_remote_warehouse.message_to_send')
        rule_pool = self.pool.get("sync.client.message_rule")
        entity = self.get_entity(cr, uid, context)

        messages_count = 0
        message_direction = entity.usb_instance_type == 'central_platform' and \
            ['|', ('direction_usb', '=', 'cp_to_rw'), ('direction_usb', '=', 'bidirectional')] or \
            ['|', ('direction_usb', '=', 'rw_to_cp'), ('direction_usb', '=', 'bidirectional')]
        rule_ids = rule_pool.search(cr, uid, [('type','=','USB')] + message_direction, order='sequence_number',  context=context)
        if rule_ids:
            for rule in rule_pool.browse(cr, uid, rule_ids, context=context):
                messages_count += message_pool.create_from_rule(cr, uid, rule, "id asc", True, context=context)
        
            if messages_count:
                cr.commit()
    
    def usb_push_validate(self, cr, uid, ids, context=None):
        """
        Update update_to_send records with new usb_sync_date
        """
        # init
        entity = self.get_entity(cr, uid, context)
        
        # check step
        if entity.usb_sync_step not in ['pull_performed','first_sync']:
            raise osv.except_osv('Cannot Validated', 'We cannot Validate the Push until we have performed one')
        
        # mark latest updates as sync finished (set usb_sync_date) and clear entity session_id
        update_pool = self.pool.get('sync_remote_warehouse.update_to_send')
        update_pool.sync_finished(cr, uid, ids, sync_field='usb_sync_date', context=context)
    
    @sync_process(step='data_pull', need_connection=False, defaults_logger={'usb':True})
    def usb_pull(self, cr, uid, uploaded_file_base64, context):
        """
        Takes the base64 for the uploaded zip file, unzips the csv files, parses them and inserts them into the database.
        @param uploaded_file_base64: The Base64 representation of a file - direct from the OpenERP api, i.e. wizard_object.pull_data
        @return: A dictionary of the CSV files enclosed in the ZIP, with their import status. Refer to STATIC error codes in this file
        """
        
        # check usb sync step
        if self.get_entity(cr, uid, context).usb_sync_step not in ['push_performed', 'first_sync']:
            raise osv.except_osv('Cannot Pull', 'We cannot perform a Pull until we have performed a Pushed')
        
        # init
        entity = self.get_entity(cr, uid)
        logger = context.get('logger', None)
        
        # prepare uploaded file
        try:
            uploaded_file = base64.decodestring(uploaded_file_base64)
            
            zip_stream = StringIO(uploaded_file)
            zip_file = ZipFile(zip_stream, 'r')
            
            md5_data = {}
            md5_data_file = zip_file.read('md5')
            for line in md5_data_file.split("\n"):
                m = re.match('^([^:]+): (.*)$', line)
                if m:
                    md5_data[m.group(1)] = m.group(2)

            # read the header if possible
            try:
                header = zip_file.read("header")
                self._check_md5(md5_data, header, 'header')
            except KeyError:
                logger.append(_("Warning: the zip-file does not contain a header"))
            else:
                header = eval(header)
                db = pooler.get_db_only(cr.dbname)
                patch_cr = db.cursor()
                try:
                    patch_cr.autocommit(True)
                    logger.append(_("Zip-file built with UniField Server %(release)s") % header)
                    revisions = self.pool.get('sync_client.version')
                    if revisions:
                        # import patches used by the zip-file
                        revisions._update(patch_cr, uid,
                            [dict(zip("name sum date importance comment".split(), p)) for p in header['patches']],
                            context=context)
                        # check database revision
                        if revisions._is_outdated(patch_cr, uid, force_recalculate=True, exact_version=True, context=context):
                            raise osv.except_osv(_("Error!"),
                                _("Your UniField Server is outdated. Please upgrade using the \"Patch Synchronization\" " \
                                  "menu under the \"Synchronization\" menu."))
                finally:
                    patch_cr.close()

            # check file validity
            file_names = self.usb_pull_files
            if entity.usb_instance_type == 'remote_warehouse':
                file_names = file_names + self.usb_pull_files_rw
            if not all([(file_name in zip_file.namelist()) for file_name in self.usb_pull_files]):
                raise osv.except_osv(_('Invalid USB Synchronisation Data'), _('The zip file you uploaded does not have all the data required for a pull. Please re-download the data from the other server.'))
            
            # import rules if RW 
            if entity.usb_instance_type == 'remote_warehouse':
                self.usb_pull_import_rules(cr, uid, zip_file, md5_data, context)
            
            # US-287: This is the quick&dirty code to increase the max size of csv reader to avoid problem of handling long text in csv file            
            maxInt = sys.maxsize
            decrement = True
            while decrement:
                # decrease the maxInt value by factor 10 as long as the OverflowError occurs.
                decrement = False
                try:
                    csv.field_size_limit(maxInt)
                except OverflowError:
                    maxInt = int(maxInt/10)
                    decrement = True
            
            # import updates
            data, updates_import_error, updates_ran, updates_run_error = self.usb_pull_import_updates(cr, uid, zip_file, md5_data, context)
            logger.switch('data_pull','ok')
            logger.switch('msg_pull','in-progress')
            messages, messages_import_error, messages_ran, messages_run_error = self.usb_pull_import_messages(cr, uid, zip_file, md5_data, context)
            
            zip_file.close()
                    
            logger.switch('status', 'ok')
        except osv.except_osv, e:
            logger.append(_(e.name + ': ' + e.value))
            logger.switch('status','failed')
            raise e
        except Exception, e:
            logger.append(_('Error while reading uploaded zip file: %s') % str(e))
            logger.switch('status', 'failed')
            raise e
    
        # return results
        return (len(data), updates_import_error, updates_ran, updates_run_error, 
                len(messages), messages_import_error, messages_ran, messages_run_error)
    
    def usb_pull_import_rules(self, cr, uid, zip_file, md5_data, context):
        update_rules = zip_file.read(self.usb_pull_update_rule_file)
        self._check_md5(md5_data, update_rules, self.usb_pull_update_rule_file)
        update_rules = eval(update_rules)
        self.pool.get('sync.client.rule').save(cr, uid, update_rules, context=context)
        
        message_rules = zip_file.read(self.usb_pull_message_rule_file)
        self._check_md5(md5_data, message_rules, self.usb_pull_message_rule_file)
        message_rules = eval(message_rules)
        self.pool.get('sync.client.message_rule').save(cr, uid, message_rules, context=context)
            
    def usb_pull_import_updates(self, cr, uid, zip_file, md5_data, context):
        logger = context.get('logger')
        logger_index = logger.append()
        
        is_first_csv_row = True
        
        data_to_import_fields = []
        data_to_import_data = []
        
        number_of_updates_ran = 0
        import_error = None
        run_error = None
        
        # get CSV object to read data_to_import_data
        file_name = '%s.csv' % self.usb_pull_update_received_model_name
        csv_file = zip_file.read(file_name)
        
        if csv_file:
            self._check_md5(md5_data, csv_file, file_name)
            csv_reader = csv.reader(StringIO(csv_file))
            # loop through csv rows and insert into data_to_import_fields or data_to_import_data array
            for row in csv_reader:
                if is_first_csv_row:
                    data_to_import_fields = row
                    is_first_csv_row = False
                else:
                    data_to_import_data.append(row)
                    
            logger.replace(logger_index, _('Update(s) to import: %d') % len(data_to_import_data))
            
            # do importation and set result[model] = [True/False, [any error messages,...]]
            model_pool = self.pool.get(self.usb_pull_update_received_model_name)
            
            try:
                model_pool.import_data(cr, uid, data_to_import_fields, data_to_import_data, context=context)
            except Exception as e:
                import_error =  '%s %s: %s' % (_('Error while importing: '), type(e), str(e))
            
            # run updates
            context.update({'update_received_model':'sync_remote_warehouse.update_received'})
            
            if not import_error:
                logger.replace(logger_index, _('Update(s) imported: %d') % len(data_to_import_data))
                try:
                    number_of_updates_ran = self.execute_updates(cr, uid, context=context)
                    self._usb_change_sync_step(cr, uid, 'pull_performed')
                    logger.switch('data_pull','ok')
                except AttributeError, e:
                    run_error = '%s: %s' % (type(e), str(e))
                    logger.append(_('Error while processing update(s): %s') % run_error)
                    logger.switch('data_pull','failed')
            else:
                logger.replace(logger_index, _('Error while importing update(s): %s') % import_error)
                logger.switch('data_pull','failed')
                
        else:
            logger.replace(logger_index, _('Update(s) to import: 0'))
            self._usb_change_sync_step(cr, uid, 'pull_performed')
            
        return (data_to_import_data, import_error, number_of_updates_ran, run_error)
    
    def usb_pull_import_messages(self, cr, uid, zip_file, md5_data, context):
        logger = context.get('logger')
        logger_index = logger.append()
        
        is_first_csv_row = True
        
        data_to_import_fields = []
        data_to_import_data = []
        
        number_of_messages_ran = 0
        import_error = None
        run_error = None
        
        # get CSV object to read data_to_import_data
        file_name = '%s.csv' % self.usb_pull_message_received_model_name
        csv_file = zip_file.read(file_name)
        
        if csv_file:
            self._check_md5(md5_data, csv_file, file_name)
            csv_reader = csv.reader(StringIO(csv_file))
            
            # loop through csv rows and insert into data_to_import_fields or data_to_import_data array
            for row in csv_reader:
                if is_first_csv_row:
                    data_to_import_fields = row
                    is_first_csv_row = False
                else:
                    data_to_import_data.append(row)
                    
            logger.replace(logger_index, _('Message(s) to import: %d') % len(data_to_import_data))
            model_pool = self.pool.get(self.usb_pull_message_received_model_name)
            
            try:
                model_pool.import_data(cr, uid, data_to_import_fields, data_to_import_data, context=context)
            except Exception as e:
                import_error =  '%s %s: %s' % (_('Error while importing: '), type(e), str(e))
            
            # run updates
            context.update({'message_received_model':'sync_remote_warehouse.message_received'})
            
            if not import_error:
                logger.replace(logger_index, _('Message(s) imported: %d') % len(data_to_import_data))
                try:
                    number_of_messages_ran = self.execute_message(cr, uid, context=context)
                    self._usb_change_sync_step(cr, uid, 'pull_performed')
                    logger.switch('msg_pull','ok')
                except AttributeError, e:
                    run_error = '%s: %s' % (type(e), str(e))
                    logger.append(_('Error while processing message(s): %s') % run_error)
                    logger.switch('msg_pull','failed')
            else:
                logger.replace(logger_index, _('Error while importing: %s') % import_error)
                logger.switch('msg_pull','failed')
        else:
            logger.replace(logger_index, _('Message(s) to import: 0'))
            logger.switch('msg_pull','ok')
            
        return (data_to_import_data, import_error, number_of_messages_ran, run_error)
        
Entity()
